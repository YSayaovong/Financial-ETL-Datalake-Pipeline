Financial-ETL-Datalake-Pipeline/
â”‚
â”œâ”€â”€ data/                        # (optional if not sharing raw data)
â”‚   â””â”€â”€ kpi_summary.csv          # Final exported dataset from Colab
â”‚
â”œâ”€â”€ SnowFlake/                  
â”‚   â”œâ”€â”€ ddl_create_tables.sql    # Table creation script
â”‚   â”œâ”€â”€ load_data_stage.sql      # Stage setup + COPY INTO commands
â”‚   â””â”€â”€ fact_kpi_summary.sql     # SELECT test query
â”‚
â”œâ”€â”€ PowerBI/                     
â”‚   â”œâ”€â”€ Financial-KPI.pbix       # Power BI dashboard file
â”‚   â””â”€â”€ visuals/                 
â”‚       â””â”€â”€ KPI_Card_Screenshot.png
â”‚       â””â”€â”€ KPI_Trend_LineChart.png
â”‚
â”œâ”€â”€ README.md                    # Final documentation
â”œâ”€â”€ .gitignore

# ðŸ’¼ Financial ETL Datalake Pipeline

This project demonstrates a full-stack ETL pipeline for financial KPI analysis using **PySpark**, **Snowflake**, and **Power BI**. It automates ingestion, transformation, and reporting of KPI data for executive dashboards.

---

## ðŸ“Œ Key Features

- Built a PySpark pipeline to process transactions, invoices, and expenses.
- Created a Snowflake database `financial_etl` with schema `kpi` and table `fact_kpi_summary`.
- Loaded clean financial KPI data using `COPY INTO` from stage.
- Visualized KPI performance in Power BI with variance cards and time-series trends.

---

## ðŸ§± Tech Stack

| Layer       | Tool            |
|-------------|-----------------|
| ETL         | Python, PySpark |
| Storage     | Snowflake       |
| Reporting   | Power BI        |
| Source Data | CSV (mock data) |

---

## ðŸ“Š Power BI Dashboard

**KPI Variance Card:**
Shows real-time variance between actual and target financial KPIs.

**Trend Line Chart:**
Visualizes `net_amount` vs `target_value` over time.

![KPI Card Screenshot](PowerBI/visuals/KPI_Card_Screenshot.png)
![Trend Chart Screenshot](PowerBI/visuals/KPI_Trend_LineChart.png)
