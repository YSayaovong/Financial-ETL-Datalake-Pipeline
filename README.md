# üìä Financial ETL Datalake Pipeline

This project simulates a **real-world ETL pipeline** for processing financial KPIs across departments. It integrates **PySpark**, **Snowflake**, and **Power BI** to ingest, transform, and visualize financial data for decision-making.

---

## üöÄ Project Overview

**Objective:** Build a scalable pipeline to process and monitor financial KPIs (e.g., cash flow, expenses, targets) across departments.

**Tools & Tech Stack:**
- `PySpark` (ETL and transformations)
- `Snowflake` (data warehouse + SQL loading)
- `Google Colab` (development environment)
- `Power BI` (visual dashboard)
- `GitHub` (version control)

---

## üß™ Key Features

- ‚úÖ Generate mock financial datasets with Faker
- üîÑ Join, clean, and transform data with PySpark
- üßä Load cleaned data into Snowflake `fact_kpi_summary` table
- üìà Visualize KPIs in Power BI: budget vs actuals, department comparisons
- üì¶ Export zipped output and SQL integration-ready tables

---

## üìÇ Folder Structure
Financial-ETL-Datalake-Pipeline/ ‚îú‚îÄ‚îÄ SnowFlake/ ‚îÇ ‚îú‚îÄ‚îÄ schema.sql # Snowflake table & schema setup ‚îÇ ‚îî‚îÄ‚îÄ copy_into.sql # Load data from stage ‚îú‚îÄ‚îÄ Screenshots/ ‚îÇ ‚îî‚îÄ‚îÄ uploaded_table_sample.png # Visual proof of Snowflake load ‚îú‚îÄ‚îÄ notebooks/ ‚îÇ ‚îî‚îÄ‚îÄ Financial_ETL_Datalake_Pipeline.ipynb ‚îî‚îÄ‚îÄ README.md


---

## üì∏ Preview

| KPI Snowflake Table | Fact KPI Summary View |
|---------------------|------------------------|
| ![financial_etl_kpi](Screenshots/financial_etl_kpi.PNG) | ![kpi_summary](Screenshots/kpi_summary.PNG) |


---

## üß† What I Learned

- Setting up a real-world Snowflake schema with staging and COPY commands
- Writing robust PySpark ETL pipelines in Colab
- Automating KPI aggregation with joins and date parsing
- Troubleshooting schema mismatches between PySpark and Snowflake
- Deploying a reproducible project from notebook ‚Üí database ‚Üí dashboard

---

## üîó Future Enhancements

- Add DBT integration for transformation layer
- Automate Snowflake uploads via Python + Streamlit
- Schedule ETL pipelines via Airflow or dbt Cloud

---

## üìå Author

**Yengkong Sayaovong**  
[LinkedIn](https://www.linkedin.com/in/ysayaovong) | [GitHub](https://github.com/YSayaovong)

---

> ‚≠ê If you found this helpful, give the repo a star!

